{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiteja15-cloud/DATA-SCIENCE/blob/main/NLP_and_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Stochastic HW4/'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kwuE_4KDPt2a",
        "outputId": "cea0e2bf-ebd4-4ed1-87f0-d5b09718267d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/My Drive/Stochastic HW4/\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZBQxNz7uoyr",
        "outputId": "f2e0444c-e8fa-450b-91ed-5f3a408c5744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "import math\n",
        "import numpy.linalg as LA\n",
        "import codecs, json\n",
        "from time import time\n",
        "import plotly.express as px\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDGj1uZgsPKY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q3: Natural Language Processing (****) (60pt)\n",
        "\n",
        "\n",
        "One of applications of Markov chain in natural language processing is language model. In particular, we will work on uni-gram and bi-gram.\n",
        "\n",
        "The following data set provides you with the training data files (a subset of the One Billion Word Language Modeling Bench-\n",
        "mark). Each line in each file contains a whitespace-tokenized sentence.\n",
        "\n",
        "- `1b benchmark.train.tokens`: data for training your language models.\n",
        "- `1b benchmark.dev.tokens`: data for debugging and choosing the best hyperparameters.\n",
        "- `1b benchmark.test.tokens`: data for evaluating your language models.\n",
        "\n",
        "\n",
        "I have precessed these dataset for the purposed of this problem. The total number of unique words is 80661. In bi-gram model, the size of the transition matrix is $80661\\times 80661$. It is impossible the store these matrices directly and we have to take advantage of the sparsity of the transition matrix.\n",
        "\n",
        "You should use the development data to choose the best values for the hyperparameters $\\lambda$. Hyperparameter\n",
        "optimization is an active area of research; for this homework, you can simply try a few combinations to find\n",
        "reasonable values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O23Noh1BuQBj",
        "outputId": "b1682ac9-df9a-4ad5-d0df-9a298b3cf2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-11 13:36:06--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8422714 (8.0M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.train.tokens’\n",
            "\n",
            "1b_benchmark.train. 100%[===================>]   8.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-11 13:36:06 (75.2 MB/s) - ‘1b_benchmark.train.tokens’ saved [8422714/8422714]\n",
            "\n",
            "--2023-11-11 13:36:06--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.dev.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1673607 (1.6M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.dev.tokens’\n",
            "\n",
            "1b_benchmark.dev.to 100%[===================>]   1.60M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-11-11 13:36:06 (22.5 MB/s) - ‘1b_benchmark.dev.tokens’ saved [1673607/1673607]\n",
            "\n",
            "--2023-11-11 13:36:06--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1650147 (1.6M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.test.tokens’\n",
            "\n",
            "1b_benchmark.test.t 100%[===================>]   1.57M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-11-11 13:36:06 (21.0 MB/s) - ‘1b_benchmark.test.tokens’ saved [1650147/1650147]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true -O 1b_benchmark.train.tokens\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.dev.tokens?raw=true -O 1b_benchmark.dev.tokens\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true -O 1b_benchmark.test.tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCkmscSwua8-"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = \"<s>\"\n",
        "STOP_TOKEN = \"</s>\"\n",
        "\n",
        "class FileParser:\n",
        "    def __init__(self, train_file=\"1b_benchmark.train.tokens\", test_file=\"1b_benchmark.test.tokens\", dev_file=\"1b_benchmark.dev.tokens\"):\n",
        "        self.TRAIN_FILE = train_file\n",
        "        self.TEST_FILE  = test_file\n",
        "        self.DEV_FILE = dev_file\n",
        "\n",
        "    def get_train_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE))\n",
        "\n",
        "    def get_dev_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.DEV_FILE))\n",
        "\n",
        "    def get_test_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE))\n",
        "\n",
        "    def get_train_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE), flatten=False)\n",
        "\n",
        "    def get_dev_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.DEV_FILE), flatten=False)\n",
        "\n",
        "    def get_test_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE), flatten=False)\n",
        "\n",
        "    def _flatten(self, l):\n",
        "        return [word for sublist in l for word in sublist]\n",
        "\n",
        "    def _tokenize(self, sentence_list, flatten=True):\n",
        "        tokenized_sentences = [re.split(\"\\s+\", sentence.strip()) for sentence in sentence_list]\n",
        "\n",
        "        if flatten:\n",
        "            return self._flatten(tokenized_sentences)\n",
        "\n",
        "        return tokenized_sentences\n",
        "\n",
        "    def _get_sentences(self, file_path):\n",
        "\n",
        "        l = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            l = f.readlines()\n",
        "\n",
        "        # Add the start and stop tokens to each sentence in the file\n",
        "        sentence_list = []\n",
        "        for sentence in l:\n",
        "            sentence_list.append(START_TOKEN + \" \" + sentence + \" \" + STOP_TOKEN)\n",
        "\n",
        "        return sentence_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvOuK1qvufdA",
        "outputId": "437c2f79-cf76-4804-d6e5-0d246b6c7488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'Abu', 'Dhabi', 'is', 'going', 'ahead', 'to', 'build', 'solar', 'city', 'and', 'no', 'pollution', 'city', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "fp = FileParser()\n",
        "train_tokens = fp.get_train_file_sentence_tokens()\n",
        "dev_tokens = fp.get_dev_file_sentence_tokens()\n",
        "test_tokens = fp.get_test_file_sentence_tokens()\n",
        "# The fourth line of training set\n",
        "print(train_tokens[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5BB2o5Zuhka",
        "outputId": "be0b1516-d406-47ff-f83d-de3e3bb1a454"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80661"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# Building the dict and the map from the words to integers for the purpose of training.\n",
        "word2idx={}\n",
        "idx2word={}\n",
        "idx=0\n",
        "for line in train_tokens:\n",
        "    for word in line:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word]=idx\n",
        "            idx2word[idx]=word\n",
        "            idx+=1\n",
        "\n",
        "# Transform the words in each sentence to integers\n",
        "train_data=[]\n",
        "for line in train_tokens:\n",
        "    train_data.append([word2idx[word] for word in line])\n",
        "\n",
        "# there are 80661 unique words.\n",
        "len(word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ8WdtDDu1DD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Your task:** Complete the code in the ```LanguageModel``` class as follows.\n",
        "\n",
        "1. (15pt) Implement **Laplace Smoothing** with the smoothing factor =1 in the function ```get_unigram_probability``` and ```get_bigram_probability```. In the bigram model, try to implement it as efficient as possible. **Hint:** For bigrams $(t_1,t_2)$ which do not occur in the sample, what is $p(t_2|t_1)$? For fixed $t_1$, are these probability the same?\n",
        "\n",
        "2. (15pt) Complete functions ```get_unigram_sentence_log_probability``` and ```get_bigram_sentence_log_probability``` to calculate the log probability of the given sentence. You need to consider the situation that when Laplace Smoothing is true and the situation that when Laplace Smoothing is false.\n",
        "\n",
        "\n",
        "3. (10pt) To make your language model work better, you will implement linear interpolation smoothing between\n",
        "unigram, bigram.\n",
        "\\begin{align}\n",
        "p'(t_2|t_1) = \\lambda_1 p(t_2) + \\lambda_2 p(t_2|t_1)\n",
        "\\end{align}\n",
        "where $p'$ represents the smoothed probability, the hyperparameters $\\lambda_1, \\lambda_2$ are weights on the unigram,\n",
        "bigram language models, respectively. So $\\lambda_1+\\lambda_2= 1$.\n",
        "Complete functions ```get_linear_interpolation_probability``` and ```get_linear_interpolation_sentence_log_probability``` with $\\mm\\lambda = (\\lambda_1, \\lambda_2)$ stored in ```linear_interpolation_factors```.\n",
        "\n",
        "4. (5pt) Testing the smoothed probability with several sentences in the development dataset. You should test when laplace smoothing is True and when laplace smoothing is False. You might find some words in testing dataset are not appeared in training set and you can set a default probability for this situation.  \n",
        "\n",
        "3. (15pt) Report the **perplexity scores** of the linear interpolation of language model for your training,\n",
        "and development sets. Report no\n",
        "more than 5 different sets of $\\lambda$. Briefly discuss the experimental results. Putting it all together, report perplexity on the test set, using the hyperparameters that you chose from\n",
        "the development set. Specify those hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel:\n",
        "    def __init__(self, laplace_smoothing=False, laplace_smoothing_factor=1, linear_interpolation_factors=(0.3, 0.7)):\n",
        "        self.unigram_freqs = {}\n",
        "        self.unigram_corpus_length = 0\n",
        "        self.num_unique_unigrams = 0\n",
        "\n",
        "        self.bigram_freqs = {}\n",
        "        self.bigram_corpus_length = 0\n",
        "        self.num_unique_bigrams = 0\n",
        "\n",
        "        self.laplace_smoothing = laplace_smoothing\n",
        "        self.laplace_smoothing_factor = laplace_smoothing_factor\n",
        "        self.linear_interpolation_factors = linear_interpolation_factors\n",
        "\n",
        "    def fit_unigram(self, tokens):\n",
        "        for clist in tokens:\n",
        "            for cindex in range(len(clist)):\n",
        "                t = clist[cindex]\n",
        "                self.unigram_freqs[t] = self.unigram_freqs.get(t, 0) + 1\n",
        "\n",
        "        self.unigram_corpus_length = sum(list(self.unigram_freqs.values()))\n",
        "        self.num_unique_unigrams = len(self.unigram_freqs)\n",
        "\n",
        "    def fit_bigram(self, tokens):\n",
        "        for clist in tokens:\n",
        "            for cindex in range(len(clist) - 1):\n",
        "                t1, t2 = clist[cindex], clist[cindex + 1]\n",
        "                self.bigram_freqs[(t1, t2)] = self.bigram_freqs.get((t1, t2), 0) + 1\n",
        "\n",
        "        self.bigram_corpus_length = sum(list(self.bigram_freqs.values()))\n",
        "        self.num_unique_bigrams = len(self.bigram_freqs)\n",
        "\n",
        "    def get_unigram_probability(self, unigram):\n",
        "\n",
        "        # With Laplace Smoothing -\n",
        "        if self.laplace_smoothing:\n",
        "            prob_numerator = self.unigram_freqs.get(unigram, 0) + self.laplace_smoothing_factor\n",
        "            prob_denominator = self.unigram_corpus_length + (self.laplace_smoothing_factor * self.num_unique_unigrams)\n",
        "\n",
        "        # Without Laplace Smooothing -\n",
        "        else:\n",
        "            prob_numerator = self.unigram_freqs.get(unigram, 0)\n",
        "            prob_denominator = self.unigram_corpus_length\n",
        "\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "    def get_bigram_probability(self, bigram):\n",
        "        t1, t2 = bigram\n",
        "        default_prob = 0.001\n",
        "        # With Laplace Smooothing -\n",
        "        if self.laplace_smoothing:\n",
        "            prob_numerator = self.bigram_freqs.get((t1, t2), 0) + self.laplace_smoothing_factor\n",
        "            prob_denominator = self.unigram_freqs.get(t1, 0) + (self.laplace_smoothing_factor * self.num_unique_unigrams)\n",
        "\n",
        "        # Without Laplace Smooothing -\n",
        "        else:\n",
        "            prob_numerator = self.bigram_freqs.get((t1, t2), 0)\n",
        "            prob_denominator = self.unigram_freqs.get(t1, 0)\n",
        "\n",
        "        if prob_denominator == 0:\n",
        "          #print(f\"Error: 'get_bigram_probability()' has a denominator of 0 for {bigram}\")\n",
        "          return default_prob\n",
        "\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "    def get_linear_interpolation_probability(self, bigram):\n",
        "        lambda_1, lambda_2 = self.linear_interpolation_factors\n",
        "        unigram_prob = self.get_unigram_probability(bigram[1])  # p(t2) SMOOTHING = TRUE\n",
        "        bigram_prob = self.get_bigram_probability(bigram)\n",
        "        return (lambda_1 * unigram_prob) + (lambda_2 * bigram_prob)\n",
        "\n",
        "    def get_unigram_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for word in sentence:\n",
        "            log_prob += math.log(self.get_unigram_probability(word)) # smoothing=self.laplace_smoothing\n",
        "        return log_prob\n",
        "\n",
        "    def get_bigram_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram = (sentence[i], sentence[i + 1])\n",
        "            log_prob += math.log(self.get_bigram_probability(bigram)) # smoothing=self.laplace_smoothing\n",
        "        return log_prob\n",
        "\n",
        "    def get_linear_interpolation_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram = (sentence[i], sentence[i + 1])\n",
        "            log_prob += math.log(self.get_linear_interpolation_probability(bigram))\n",
        "        return log_prob\n",
        "\n",
        "    def get_unigram_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_unigram_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "    def get_bigram_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_bigram_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "    def get_linear_interpolation_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_linear_interpolation_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "# Ref - Class notes, Course Github codes"
      ],
      "metadata": {
        "id": "0g1jxI_5jd_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4\n",
        "\n",
        "lm_with_smoothing = LanguageModel(laplace_smoothing=True, laplace_smoothing_factor=1)\n",
        "lm_without_smoothing = LanguageModel()\n",
        "\n",
        "lm_with_smoothing.fit_unigram(train_data)\n",
        "lm_with_smoothing.fit_bigram(train_data)\n",
        "\n",
        "lm_without_smoothing.fit_unigram(train_data)\n",
        "lm_without_smoothing.fit_bigram(train_data)\n",
        "\n",
        "print(\"Unigram Probabilities without smoothing-\")\n",
        "for sentence in dev_tokens[1:2]:\n",
        "    for word in sentence:\n",
        "        prob_without_smoothing = lm_without_smoothing.get_unigram_probability(word)\n",
        "        print(f\"Word: {word}, Probability without Smoothing: {prob_without_smoothing}\")\n",
        "\n",
        "print(\"\\nUnigram probabilities with smoothing-\")\n",
        "for sentence in dev_tokens[1:2]:\n",
        "    for word in sentence:\n",
        "        prob_with_smoothing = lm_with_smoothing.get_unigram_probability(word)\n",
        "        print(f\"Word: {word}, Probability with Smoothing: {prob_with_smoothing}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0g20m07eGOT",
        "outputId": "9da2064e-6310-4395-c7fc-0981e2d1c5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities without smoothing-\n",
            "Word: <s>, Probability without Smoothing: 0.0\n",
            "Word: Everybody, Probability without Smoothing: 0.0\n",
            "Word: is, Probability without Smoothing: 0.0\n",
            "Word: aware, Probability without Smoothing: 0.0\n",
            "Word: of, Probability without Smoothing: 0.0\n",
            "Word: consequences, Probability without Smoothing: 0.0\n",
            "Word: ., Probability without Smoothing: 0.0\n",
            "Word: </s>, Probability without Smoothing: 0.0\n",
            "\n",
            "Unigram probabilities with smoothing-\n",
            "Word: <s>, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: Everybody, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: is, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: aware, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: of, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: consequences, Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: ., Probability with Smoothing: 5.665414232427018e-07\n",
            "Word: </s>, Probability with Smoothing: 5.665414232427018e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bigram probabilities with smoothing-\")\n",
        "for sentence in dev_tokens[1:2]:\n",
        "    for i in range(len(sentence) - 1):\n",
        "        bigram = (sentence[i], sentence[i + 1])\n",
        "        prob_without_smoothing = lm_without_smoothing.get_bigram_probability(bigram)\n",
        "        print(f\"Bigram: {bigram}, Probability without Smoothing: {prob_without_smoothing}\")\n",
        "\n",
        "print(\"\\nBigram probabilities with smoothing-\")\n",
        "for sentence in dev_tokens[1:2]:\n",
        "    for i in range(len(sentence) - 1):\n",
        "        bigram = (sentence[i], sentence[i + 1])\n",
        "        prob_with_smoothing = lm_with_smoothing.get_bigram_probability(bigram)\n",
        "        print(f\"Bigram: {bigram}, Probability without Smoothing: {prob_with_smoothing}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ziow73-eYeW",
        "outputId": "f2252e2a-24f0-4c5e-f47c-7f1a40be2870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram probabilities with smoothing-\n",
            "Bigram: ('<s>', 'Everybody'), Probability without Smoothing: 0.001\n",
            "Bigram: ('Everybody', 'is'), Probability without Smoothing: 0.001\n",
            "Bigram: ('is', 'aware'), Probability without Smoothing: 0.001\n",
            "Bigram: ('aware', 'of'), Probability without Smoothing: 0.001\n",
            "Bigram: ('of', 'consequences'), Probability without Smoothing: 0.001\n",
            "Bigram: ('consequences', '.'), Probability without Smoothing: 0.001\n",
            "Bigram: ('.', '</s>'), Probability without Smoothing: 0.001\n",
            "\n",
            "Bigram probabilities with smoothing-\n",
            "Bigram: ('<s>', 'Everybody'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('Everybody', 'is'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('is', 'aware'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('aware', 'of'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('of', 'consequences'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('consequences', '.'), Probability without Smoothing: 1.2397565118210784e-05\n",
            "Bigram: ('.', '</s>'), Probability without Smoothing: 1.2397565118210784e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5\n",
        "\n",
        "lm_linear_interpolation = LanguageModel(laplace_smoothing=True, laplace_smoothing_factor=1)\n",
        "lm_linear_interpolation.fit_unigram(train_data)\n",
        "lm_linear_interpolation.fit_bigram(train_data)\n",
        "\n",
        "# Define different sets of lambda values\n",
        "lambda_sets = [(0.2, 0.8), (0.4, 0.6), (0.5, 0.5), (0.6, 0.4), (0.8, 0.2)]\n",
        "\n",
        "# Compute perplexity scores for each set of lambda values on the training and development sets\n",
        "for lambdas in lambda_sets:\n",
        "    lm_linear_interpolation.linear_interpolation_factors = lambdas\n",
        "\n",
        "    train_perplexity = lm_linear_interpolation.get_linear_interpolation_perplexity(train_data)\n",
        "    dev_perplexity = lm_linear_interpolation.get_linear_interpolation_perplexity(dev_tokens)\n",
        "\n",
        "    print(f\"Lambda values: {lambdas}\")\n",
        "    print(f\"Training Perplexity: {train_perplexity}\")\n",
        "    print(f\"Development Perplexity: {dev_perplexity}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raQGHtnVkUnW",
        "outputId": "21f8a2b8-5c4a-49c0-8d3a-988a1edfeb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambda values: (0.2, 0.8)\n",
            "Training Perplexity: 1366.0797982191657\n",
            "Development Perplexity: 65511.71145427921\n",
            "\n",
            "\n",
            "Lambda values: (0.4, 0.6)\n",
            "Training Perplexity: 1132.2554569040485\n",
            "Development Perplexity: 84897.75398931614\n",
            "\n",
            "\n",
            "Lambda values: (0.5, 0.5)\n",
            "Training Perplexity: 1078.6091091879275\n",
            "Development Perplexity: 99781.25404446606\n",
            "\n",
            "\n",
            "Lambda values: (0.6, 0.4)\n",
            "Training Perplexity: 1048.2493245779701\n",
            "Development Perplexity: 121165.60100657163\n",
            "\n",
            "\n",
            "Lambda values: (0.8, 0.2)\n",
            "Training Perplexity: 1048.369270521507\n",
            "Development Perplexity: 214251.55663371072\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the lowest development perplexity score\n",
        "best_lambda_values = (0.2, 0.8)\n",
        "lm_linear_interpolation.linear_interpolation_factors = best_lambda_values\n",
        "\n",
        "# Compute perplexity on the test set\n",
        "test_perplexity = lm_linear_interpolation.get_linear_interpolation_perplexity(test_tokens)\n",
        "print(f\"Best Lambda Values: {best_lambda_values}\")\n",
        "print(f\"Test Perplexity: {test_perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezM-xc0zlFC1",
        "outputId": "9c0ad72c-f55c-4258-f59f-aafdc24bc462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Lambda Values: (0.2, 0.8)\n",
            "Test Perplexity: 65387.9848265971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyaTm4mLbkiU"
      },
      "source": [
        "# Q4: (Bonus) Beam Search Revisited (*****) (15pt)\n",
        "Implement Beam Search Algorithm for Q3. Note you need to use various data structure and graphs to reduce the complexity of implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import heapq\n",
        "\n",
        "class LanguageModel:\n",
        "    def __init__(self, laplace_smoothing=False, laplace_smoothing_factor=1, linear_interpolation_factors=(0.3, 0.7)):\n",
        "        self.unigram_freqs = {}\n",
        "        self.unigram_corpus_length = 0\n",
        "        self.num_unique_unigrams = 0\n",
        "\n",
        "        self.bigram_freqs = {}\n",
        "        self.bigram_corpus_length = 0\n",
        "        self.num_unique_bigrams = 0\n",
        "\n",
        "        self.laplace_smoothing = laplace_smoothing\n",
        "        self.laplace_smoothing_factor = laplace_smoothing_factor\n",
        "        self.linear_interpolation_factors = linear_interpolation_factors\n",
        "\n",
        "    def fit_unigram(self, tokens):\n",
        "        for clist in tokens:\n",
        "            for cindex in range(len(clist)):\n",
        "                t = clist[cindex]\n",
        "                self.unigram_freqs[t] = self.unigram_freqs.get(t, 0) + 1\n",
        "\n",
        "        self.unigram_corpus_length = sum(list(self.unigram_freqs.values()))\n",
        "        self.num_unique_unigrams = len(self.unigram_freqs)\n",
        "\n",
        "    def fit_bigram(self, tokens):\n",
        "        for clist in tokens:\n",
        "            for cindex in range(len(clist) - 1):\n",
        "                t1, t2 = clist[cindex], clist[cindex + 1]\n",
        "                self.bigram_freqs[(t1, t2)] = self.bigram_freqs.get((t1, t2), 0) + 1\n",
        "\n",
        "        self.bigram_corpus_length = sum(list(self.bigram_freqs.values()))\n",
        "        self.num_unique_bigrams = len(self.bigram_freqs)\n",
        "\n",
        "    def get_unigram_probability(self, unigram):\n",
        "\n",
        "        # With Laplace Smoothing -\n",
        "        if self.laplace_smoothing:\n",
        "            prob_numerator = self.unigram_freqs.get(unigram, 0) + self.laplace_smoothing_factor\n",
        "            prob_denominator = self.unigram_corpus_length + (self.laplace_smoothing_factor * self.num_unique_unigrams)\n",
        "\n",
        "        # Without Laplace Smooothing -\n",
        "        else:\n",
        "            prob_numerator = self.unigram_freqs.get(unigram, 0)\n",
        "            prob_denominator = self.unigram_corpus_length\n",
        "\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "    def get_bigram_probability(self, bigram):\n",
        "        t1, t2 = bigram\n",
        "\n",
        "        # With Laplace Smooothing -\n",
        "        if self.laplace_smoothing:\n",
        "            prob_numerator = self.bigram_freqs.get((t1, t2), 0) + self.laplace_smoothing_factor\n",
        "            prob_denominator = self.unigram_freqs.get(t1, 0) + (self.laplace_smoothing_factor * self.num_unique_unigrams)\n",
        "\n",
        "        # Without Laplace Smooothing -\n",
        "        else:\n",
        "            prob_numerator = self.bigram_freqs.get((t1, t2), 0)\n",
        "            prob_denominator = self.unigram_freqs.get(t1, 0)\n",
        "\n",
        "        if prob_denominator == 0:\n",
        "          print(f\"Error: 'get_bigram_probability()' has a denominator of 0 for {bigram}\")\n",
        "          return 0\n",
        "\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "    def get_linear_interpolation_probability(self, bigram):\n",
        "        lambda_1, lambda_2 = self.linear_interpolation_factors\n",
        "        unigram_prob = self.get_unigram_probability(bigram[1])  # p(t2) SMOOTHING = TRUE\n",
        "        bigram_prob = self.get_bigram_probability(bigram)\n",
        "        return (lambda_1 * unigram_prob) + (lambda_2 * bigram_prob)\n",
        "\n",
        "\n",
        "    def get_unigram_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for word in sentence:\n",
        "            log_prob += math.log(self.get_unigram_probability(word)) # smoothing=self.laplace_smoothing\n",
        "        return log_prob\n",
        "\n",
        "    def get_bigram_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram = (sentence[i], sentence[i + 1])\n",
        "            log_prob += math.log(self.get_bigram_probability(bigram)) # smoothing=self.laplace_smoothing\n",
        "        return log_prob\n",
        "\n",
        "    def get_linear_interpolation_sentence_log_probability(self, sentence):\n",
        "        log_prob = 0.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram = (sentence[i], sentence[i + 1])\n",
        "            log_prob += math.log(self.get_linear_interpolation_probability(bigram))\n",
        "        return log_prob\n",
        "\n",
        "    def get_unigram_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_unigram_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "    def get_bigram_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_bigram_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "    def get_linear_interpolation_perplexity(self, sentences):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            total_words += len(sentence)\n",
        "            total_log_prob += self.get_linear_interpolation_sentence_log_probability(sentence)\n",
        "        return math.exp(-total_log_prob / total_words)\n",
        "\n",
        "    def beam_search(self, start_token='<s>', stop_token='</s>', beam_width=5, max_length=20):\n",
        "        beam = [([], 0.0)]  # (hypothesis, log probability)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            candidates = []\n",
        "\n",
        "            for hypothesis, log_prob in beam:\n",
        "                current_word = hypothesis[-1] if hypothesis else start_token\n",
        "\n",
        "                if current_word == stop_token:\n",
        "                    candidates.append((hypothesis, log_prob))\n",
        "                    continue\n",
        "\n",
        "                next_words = self.get_next_words(current_word, beam_width)\n",
        "                for next_word in next_words:\n",
        "                    new_hypothesis = hypothesis + [next_word]\n",
        "                    new_log_prob = log_prob + math.log(self.get_linear_interpolation_probability((current_word, next_word)))\n",
        "\n",
        "                    candidates.append((new_hypothesis, new_log_prob))\n",
        "\n",
        "            beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[1])\n",
        "\n",
        "        return beam\n",
        "\n",
        "    def get_next_words(self, current_word, beam_width):\n",
        "        word_probabilities = [(word, self.get_linear_interpolation_probability((current_word, word))) for word in self.unigram_freqs.keys()]\n",
        "        sorted_words = sorted(word_probabilities, key=lambda x: x[1], reverse=True)\n",
        "        next_words = [word for word, _ in sorted_words[:beam_width]]\n",
        "        return next_words"
      ],
      "metadata": {
        "id": "QtMi5LNoCLrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ref - Class notes, Course Github codes, Algorithms and Data Structures\n",
        "lm = LanguageModel()\n",
        "lm.fit_unigram(train_tokens)\n",
        "lm.fit_bigram(train_tokens)\n",
        "\n",
        "beam_width = 5\n",
        "max_length = 10\n",
        "lm.beam_search(start_token='fly', stop_token='</s>', beam_width=beam_width, max_length=max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPBn9YEHOf8S",
        "outputId": "d1008dc2-f4f1-4e8f-e879-4c05bf5d1b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['.', '</s>'], -2.9190886823857496),\n",
              " (['to', 'the', ',', '\"', '</s>'], -15.244650470702698),\n",
              " (['to', 'the', ',', '\"', 'said', '.', '</s>'], -17.911989531129805),\n",
              " (['to', 'the', ',', '\"', 'he', 'said', '.', '</s>'], -20.229702211080788),\n",
              " (['to', 'the', ',', '\"', 'he', 'said', '.', ',', 'the', 'the'],\n",
              "  -31.065786721335)]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sV4XAs2e7WV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}